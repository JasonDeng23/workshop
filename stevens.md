## Questions

1. In the reading today (Stevens et al.) the authors use a technique to produce a high resolution description of the distribution of human populations across the globe. What is the name of the technique and describe in general and basic terms how it works? 

2. The random forest method used by the authors is a machine learning algorithm (ensemble method). In general terms, what is a machine learning algorithm? Within the context of this study what distinguishes a data science, machine learning method (such as random forest) from previous classical statistical approaches to describing and analyzing phenomenon and events?

3. In the reading, the authors use a number of geospatial covariates as predictors in their machine learning method. What were these geospatial covariates and approximately how big of a data set did they represent (in general terms)? What is the significance of big data in the estimation of machine learning methods for inferring the correlates and drivers of human population distributions?

4. The authorsâ€™ results present a remarkable improvement over previous geospatial descriptions at very high resolution, of the distribution of the human population. Within the context of human development in LMICs, what is the significance of having a highly accurate description of where each person is located across planet earth?

5. Within the context of human development in LMICs, what is the relevance to your area of investigation in having a highly accurate description of where each household and person is located across planet earth?

## Responses

The authors use a technique called the random forest model in order to produce a high resolution description of the distribution of human populations across the globe. A random forest model grows a "forest" of individiaul classification or regression trees which can then be used to determine predictions through machine learning models. 

A machine learning algorithm is a computational process that adapts and predicts with many covariates and variables in order to develop and determine and accurate output. It's a general umbrella term that basically describes any machine / process that can identify patterns, usually with artificial intelligence. This differentiates from pervious classical statistical approaches because it allows for better interpolation of vast amounts of data with many covariates. Additionally, it allows us to better predict areas that might lack data or are difficult to manually determine. Thus, the ability for algorithims and computation to predict such becames essential.

The authors used covariates like night-time lights, slopes, facilities (like hospitals or schools), etc. In general, they were of considerable size, some more or some less. For example, some data sets had grid cells bigger than 100m, which suggests that it may not be as big and fine as other sets. Big data is significant in machine learning models because bringing together large amounts of data allows for better prediction for machine learning models as it allows the "biggest" picture and interpretation of the world. 

Being able to better predict and depict where each person is located across the planet has a wealth of implications. Just a few include better disease tracking and control, better distribution of schools and health facilities, better distribution of roads and other public infrastructure, etc. Human development becomes much more managable with an accurate understanding of where the people are.

Epidemiology relies heavily on being able to see where people are living and how they move. Vector borne diseases are often difficult to track based on how often and how many people move a year. With accurate descriptions of households of persons across the planet, we can not only see where to deliver supplies in case of emergency or where to develop hospitals, but also we can use the population data to track and predict precisely how people will move/travel. 
